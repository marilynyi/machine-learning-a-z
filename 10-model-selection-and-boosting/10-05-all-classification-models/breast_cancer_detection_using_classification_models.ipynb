{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8FLud1n-3pVm"
   },
   "source": [
    "# Breast Cancer Detection using Classification Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Scenario**: You receive a dataset which includes various cytological features on breast masses and whether the mass is benign (non-cancerous) or malignant (cancerous).\n",
    "\n",
    "**Goal**: Develop prediction models using all classification models introduced in the course to determine whether a new mass being diagnosed for breast cancer is benign or malignant.\n",
    "\n",
    "**Results**: \n",
    "\n",
    "The dataset has 683 rows and nine cytological features, in which 65% of the masses are benign and the remaining 35% are malignant.\n",
    "\n",
    "The dependent variable `Class` was label encoded to provide more intuitive values and because the XGBoost module cannot properly read the default values. The benign class value of `2` was relabeled `0` and the malignant class value of `4` was relabeled `1`.\n",
    "\n",
    "All the classification models used and their results are shown below in descending order of Accuracy (w/ k-Fold CV).\n",
    "\n",
    "| Model | Confusion matrix |  Accuracy Score<br>(single test set) | Accuracy<br>(w/ k-Fold CV) | Standard Deviation<br>(w/ k-Fold CV) |\n",
    "| :-- | :--: | :--: | :--: | :--: |\n",
    "| Support Vector Machine (SVM) | `[83 4]`<br>`[2 48]` | 0.956 | 97.07% | 2.19% |\n",
    "| XGBoost | `[84 3]`<br>`[1 49]` | 0.9708 | 96.89% | 2.17% |\n",
    "| Kernel SVM | `[82 5]`<br>`[1 49]` | 0.9562 | 96.89% | 2.17% |\n",
    "| K-Nearest Neighbors (K-NN) | `[83 4]`<br>`[2 48]` | 0.9562 | 96.70% | 1.79% |\n",
    "| Logistic Regression | `[84 3]`<br>`[3 47]` | 0.9562 | 96.70% | 1.97% |\n",
    "| CatBoost | `[84 3]`<br>`[0 50]` | 0.9781 | 96.53% | 2.50% |\n",
    "| Naive Bayes | `[80 7]`<br>`[0 50]` | 0.9489 | 96.52% | 2.24% |\n",
    "| Random Forest | `[83 4]`<br>`[3 47]` | 0.9489 | 96.34% | 2.16% |\n",
    "| Decision Tree | `[84 3]`<br>`[3 47]` | 0.9562 | 94.33% | 2.65% |\n",
    "\n",
    "Features scaling was applied to normalize values in the logistic regression, K-NN, SVM, and kernel SVM models. This had no effect on the other models as they are insensitive to feature scaling.\n",
    "\n",
    "Comparing all the models, the CatBoost model had the highest accuracy of 97.8% on the single test set. This model as well as Naive Bayes predicted no false negatives. However, the Naive Bayes model predicted a large amount of false positives.\n",
    "\n",
    "With k=10 cross validation, the SVM model had the highest accuracy. While all accuracies tend to be near 96% or above, the lowest performing model is Decision Tree at 94.33% accuracy.\n",
    "\n",
    "Note that all models were not tuned for optimal parameter values except CatBoost which is self-tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sO8VPU6n3vES"
   },
   "source": [
    "## Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "clDSsF7P33NU"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zGpwK5XD386E"
   },
   "source": [
    "## Importing the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We disregard the first column `Sample code number` as an independent variable since it only identifies the patient and provides no substantial measure in assessing whether the cancer is benign or malignant. Therefore, the subset `X` does not include this column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Zcksk88u4Ae8"
   },
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('Data.csv')\n",
    "X = dataset.iloc[:, 1:-1].values\n",
    "y = dataset.iloc[:, -1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "683"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset has 683 rows of data. Since this is a smaller dataset, we'll use an 80/20 split for the training set and test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label encoding the dependent variable "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dependent variable `y` is the `Class` that indicates whether the cancer is benign (`Class` = `2`) or malignant (`Class` = `4).\n",
    "\n",
    "Because these values are non-intuitive at first glance and the XGBoost model cannot read these values properly, we use label encoding to transform the dependent variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 1 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "print(y[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The encoding performed the following:\n",
    "- benign `Class = 2` -> `Class = 0`\n",
    "- malignant `Class = 4` -> `Class = 1`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XNn2RnST6_Q-"
   },
   "source": [
    "## Splitting the dataset into the Training set and Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ajhBL-er7Gry"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "546"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "137"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For consistency, we apply feature scaling to all the classification models to ensure all features are normalized to weigh them equally. Note that many of the models are insensitive to feature scaling like the naive Bayes, decision tree, random forest, XGBoost, and CatBoost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining each classification model as a function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compare results between the classification models, we define each model as functions to call them more easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Logistic Regression\n",
    "def log_reg():\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    classifier = LogisticRegression(random_state = 0)\n",
    "    classifier.fit(X_train, y_train)\n",
    "    return classifier\n",
    "\n",
    "# 2. K-Nearest Neighbors\n",
    "def k_nn():\n",
    "    from sklearn.neighbors import KNeighborsClassifier\n",
    "    # p = 2 for Euclidean (p = 1 for Manhattan)\n",
    "    classifier = KNeighborsClassifier(n_neighbors = 5, metric = 'minkowski', p = 2)\n",
    "    classifier.fit(X_train, y_train)\n",
    "    return classifier\n",
    "\n",
    "# 3. Support Vector Machine\n",
    "def svm():\n",
    "    from sklearn.svm import SVC\n",
    "    classifier = SVC(kernel = 'linear', random_state = 0)\n",
    "    classifier.fit(X_train, y_train)\n",
    "    return classifier\n",
    "\n",
    "# 4. Kernel SVM\n",
    "def kernel_svm():\n",
    "    from sklearn.svm import SVC\n",
    "    classifier = SVC(kernel = 'rbf', random_state = 0)\n",
    "    classifier.fit(X_train, y_train)   \n",
    "    return classifier\n",
    "\n",
    "# 5. Naive Bayes\n",
    "def naive_bayes():\n",
    "    from sklearn.naive_bayes import GaussianNB\n",
    "    classifier = GaussianNB()\n",
    "    classifier.fit(X_train, y_train)\n",
    "    return classifier\n",
    "\n",
    "# 6. Decision Tree\n",
    "def decision_tree():\n",
    "    from sklearn.tree import DecisionTreeClassifier\n",
    "    classifier = DecisionTreeClassifier(criterion = 'entropy', random_state = 0)\n",
    "    classifier.fit(X_train, y_train)\n",
    "    return classifier\n",
    "\n",
    "# 7. Random Forest\n",
    "def random_forest():\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    classifier = RandomForestClassifier(n_estimators = 10, criterion = 'entropy', random_state = 0)\n",
    "    classifier.fit(X_train, y_train)\n",
    "    return classifier\n",
    "\n",
    "# 8. XGBoost\n",
    "def xgboost():\n",
    "    from xgboost import XGBClassifier\n",
    "    classifier = XGBClassifier()\n",
    "    classifier.fit(X_train, y_train)\n",
    "    return classifier\n",
    "\n",
    "# 9. CatBoost\n",
    "def catboost():\n",
    "    from catboost import CatBoostClassifier\n",
    "    # Suppress iteration output with 'Silent' logging level\n",
    "    classifier = CatBoostClassifier(logging_level=\"Silent\")\n",
    "    classifier.fit(X_train, y_train)\n",
    "    return classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ivqmubzW7dFJ"
   },
   "source": [
    "## Making the Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SUSZ3zm_7gRD"
   },
   "outputs": [],
   "source": [
    "def confusion_matrix(classifier):\n",
    "    from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "    y_pred = classifier.predict(X_test)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    print(f\"Confusion matrix:\\n{cm}\")\n",
    "    accuracy_score(y_test, y_pred)\n",
    "    acc_score = accuracy_score(y_test, y_pred)\n",
    "    print(f\"Accuracy score: {acc_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EnbCjHgQ8XPn"
   },
   "source": [
    "## Applying k-Fold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yYbfiITD8ZAz"
   },
   "outputs": [],
   "source": [
    "def k_fold_cv(classifier):\n",
    "    cv = 10\n",
    "    from sklearn.model_selection import cross_val_score\n",
    "    accuracies = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv = cv)\n",
    "    print(f\"With {cv}-fold cross val:\")\n",
    "    print(f\"    Accuracy: {accuracies.mean()*100:.2f}\")\n",
    "    print(f\"    Standard Deviation: {accuracies.std()*100:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Displaying performance results of each classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Logistic Regression\n",
      "Confusion matrix:\n",
      "[[84  3]\n",
      " [ 3 47]]\n",
      "Accuracy score: 0.9562043795620438\n",
      "With 10-fold cross val:\n",
      "    Accuracy: 96.70\n",
      "    Standard Deviation: 1.97\n",
      "------------------------------\n",
      "Model: K-Nearest Neighbors\n",
      "Confusion matrix:\n",
      "[[83  4]\n",
      " [ 2 48]]\n",
      "Accuracy score: 0.9562043795620438\n",
      "With 10-fold cross val:\n",
      "    Accuracy: 96.70\n",
      "    Standard Deviation: 1.79\n",
      "------------------------------\n",
      "Model: Support Vector Machine\n",
      "Confusion matrix:\n",
      "[[83  4]\n",
      " [ 2 48]]\n",
      "Accuracy score: 0.9562043795620438\n",
      "With 10-fold cross val:\n",
      "    Accuracy: 97.07\n",
      "    Standard Deviation: 2.19\n",
      "------------------------------\n",
      "Model: Kernel SVM\n",
      "Confusion matrix:\n",
      "[[82  5]\n",
      " [ 1 49]]\n",
      "Accuracy score: 0.9562043795620438\n",
      "With 10-fold cross val:\n",
      "    Accuracy: 96.89\n",
      "    Standard Deviation: 2.17\n",
      "------------------------------\n",
      "Model: Naive Bayes\n",
      "Confusion matrix:\n",
      "[[80  7]\n",
      " [ 0 50]]\n",
      "Accuracy score: 0.948905109489051\n",
      "With 10-fold cross val:\n",
      "    Accuracy: 96.52\n",
      "    Standard Deviation: 2.24\n",
      "------------------------------\n",
      "Model: Decision Tree\n",
      "Confusion matrix:\n",
      "[[84  3]\n",
      " [ 3 47]]\n",
      "Accuracy score: 0.9562043795620438\n",
      "With 10-fold cross val:\n",
      "    Accuracy: 94.33\n",
      "    Standard Deviation: 2.65\n",
      "------------------------------\n",
      "Model: Random Forest\n",
      "Confusion matrix:\n",
      "[[83  4]\n",
      " [ 3 47]]\n",
      "Accuracy score: 0.948905109489051\n",
      "With 10-fold cross val:\n",
      "    Accuracy: 96.34\n",
      "    Standard Deviation: 2.16\n",
      "------------------------------\n",
      "Model: XGBoost\n",
      "Confusion matrix:\n",
      "[[84  3]\n",
      " [ 1 49]]\n",
      "Accuracy score: 0.9708029197080292\n",
      "With 10-fold cross val:\n",
      "    Accuracy: 96.89\n",
      "    Standard Deviation: 2.17\n",
      "------------------------------\n",
      "Model: CatBoost\n",
      "Confusion matrix:\n",
      "[[84  3]\n",
      " [ 0 50]]\n",
      "Accuracy score: 0.9781021897810219\n",
      "With 10-fold cross val:\n",
      "    Accuracy: 96.53\n",
      "    Standard Deviation: 2.50\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "models = {\n",
    "    \"Logistic Regression\": log_reg(), \n",
    "    \"K-Nearest Neighbors\": k_nn(),\n",
    "    \"Support Vector Machine\": svm(), \n",
    "    \"Kernel SVM\": kernel_svm(),\n",
    "    \"Naive Bayes\": naive_bayes(),\n",
    "    \"Decision Tree\": decision_tree(),\n",
    "    \"Random Forest\": random_forest(),\n",
    "    \"XGBoost\": xgboost(),\n",
    "    \"CatBoost\": catboost()\n",
    "}\n",
    "\n",
    "for model_name, model_function in models.items():\n",
    "    print(f\"Model: {model_name}\")\n",
    "    classifier = model_function\n",
    "    confusion_matrix(classifier)\n",
    "    k_fold_cv(classifier)\n",
    "    print(\"-\"*30)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "xg_boost.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
